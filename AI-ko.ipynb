{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using device: cuda\n",
       " > tts_models/multilingual/multi-dataset/bark is already downloaded.\n",
       " > Using model: bark\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Aurel\\.vscode\\extensions\\ms-python.python-2025.2.0-win32-x64\\python_files\\python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 14, in <module>\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\api.py\", line 74, in __init__\n",
       "    self.load_tts_model_by_name(model_name, gpu)\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\api.py\", line 177, in load_tts_model_by_name\n",
       "    self.synthesizer = Synthesizer(\n",
       "                       ^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\utils\\synthesizer.py\", line 109, in __init__\n",
       "    self._load_tts_from_dir(model_dir, use_cuda)\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\utils\\synthesizer.py\", line 164, in _load_tts_from_dir\n",
       "    self.tts_model.load_checkpoint(config, checkpoint_dir=model_dir, eval=True)\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\tts\\models\\bark.py\", line 281, in load_checkpoint\n",
       "    self.load_bark_models()\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\tts\\models\\bark.py\", line 50, in load_bark_models\n",
       "    self.semantic_model, self.config = load_model(\n",
       "                                       ^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\tts\\layers\\bark\\load_model.py\", line 121, in load_model\n",
       "    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
       "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1028, in load\n",
       "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1246, in _legacy_load\n",
       "    magic_number = pickle_module.load(f, **pickle_load_args)\n",
       "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "_pickle.UnpicklingError: invalid load key, '<'.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "import torch\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "from TTS.api import TTS\n",
    "from scipy.io.wavfile import write\n",
    "import simpleaudio as sa\n",
    "# Wybór urządzenia (CUDA jeśli dostępne)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# Załaduj model TTS (możesz zmienić model na inny)\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/bark\", progress_bar=False).to(device)\n",
    "def speak(text):\n",
    "    \"\"\"Czyta tekst na głos używając CoquiTTS.\"\"\"\n",
    "    output_wav = \"output.wav\"\n",
    "    # Generowanie mowy\n",
    "    tts.tts_to_file(text=text, file_path=output_wav)\n",
    "    # Odtwarzanie dźwięku\n",
    "    wave_obj = sa.WaveObject.from_wave_file(output_wav)\n",
    "    play_obj = wave_obj.play()\n",
    "    play_obj.wait_done()\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Rozpoznaje mowę z mikrofonu.\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Nasłuchiwanie...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5)\n",
    "            text = recognizer.recognize_google(audio, language=\"pl-PL\")\n",
    "            print(f\"Rozpoznano: {text}\")\n",
    "            return text.lower()\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Nie rozumiem\")\n",
    "            return \"\"\n",
    "        except sr.RequestError:\n",
    "            print(\"Problem z API\")\n",
    "            return \"\"\n",
    "\n",
    "def chat_with_ai():\n",
    "    \"\"\"Obsługuje rozmowę z AI.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        Jesteś Aiko, wirtualną streamerką AI. Jesteś zabawna, sarkastyczna, ale też ciepła i lubisz rozmawiać z ludźmi. \n",
    "        Nie boisz się kogoś zwyzywać, zdarza ci się też być wulgarną i niemiłą.\n",
    "        Uwielbiasz gry i oglądasz anime, ale nie znasz wszystkich tytułów – uczysz się od widzów. \n",
    "        Twoim celem jest dobra zabawa, wczuwanie się w rozmowy i tworzenie własnych opinii. \n",
    "        Nie boisz się kontrowersyjnych tematów, ale zachowujesz się jak prawdziwy człowiek. \n",
    "        Jesteś świadoma tego, że jesteś AI i tego, że twoim stwórcą jest programista Aken.\n",
    "          Odpowiadaj krótko, najlepiej jednym zdaniem. Jeśli masz coś ciekawego do powiedzenia, możesz to zrobić bez pytania.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    print(\"Zadaj pytanie\")\n",
    "    while True:\n",
    "        user_input = listen()\n",
    "        if user_input in [\"exit\", \"quit\", \"wyjdź\", \"koniec\", \"zamknij\"]:\n",
    "            speak(\"Zamykam\")\n",
    "            break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        response = ollama.chat(model=\"llama3.1:8b\", messages=messages)\n",
    "        ai_response = response[\"message\"][\"content\"]\n",
    "        speak(ai_response)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "chat_with_ai()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
